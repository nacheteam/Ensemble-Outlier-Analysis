# Notas Outlier Analysis Charu C. Aggarwal 2017

## Cap√≠tulo 1 An Introduction to Oulier Analysis

### 1.1 Introduction
An outlier or anomaly could be defined as an observation that deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanisim.

The outlier detection can be used in: IDS, credit card fraud detection, interesting sensor events, medical diagnosis, law enforcement or earth science.

The normal data points are usually referred as inliers. In some cases the ouliers are not single points but a sequence or group of points that represents by themselves an anomaly.

When detecting anomalies algorithms can give two types of output: outlier scores that gives a score qualifying the outlierness of each point and binary labels indicating whether a point is an outlier or not.

It is important to make the difference between anomaly and outlier. An outlier refers to a point that can be considered as noise or an abnormality and anomaly refers to a special kind of outlier that is of interest of the analyst.

We could fall on the idea of considering the noise as low scored (weak) outliers and anomaly as high scored (strong) ones, but the real barrier between those two is the interest of the analyst.


Unsupervised detection is the one in which we dont have previous examples and normal data to compare with the new ones and supervised detection is the opposite one.

It is clear that the dimensionality, the type of attributes in the data or the ordering are really important when detecting anomalies and should be taken into account.

### 1.2 The Data Model is Everything
The data model chosen to fit our data is crucial as it will give us by methods as k-nearest neighbors or studying the deviation from normal points a score of the outlierness.

It is a really dificult task to choose the model as it is an unsupervised one in which we dont have labeled examples to learn from. Because of this the model is usually chosen by the analyst on what he believes it is the best.

A simple model to qualify a point as an outlier could be the Z-test. This test computes with the obervations $X_1 , ... , X_N$ with mean $\mu$ and standard deviation $\sigma$ the number $Z_i = \frac{|X_i - \mu|}{\sigma}$. An implicit asumption of this is that the model is a Gaussian one. A simple classification could be that if $Z_i \geq 3$ then we classify the point as an anomaly.

If there are not enough data points the mean and deviation cannot be obtained robustly and we should be considering other distribution (Student's one) rather tahn normal one.

The model chosen depends on the data, for example if they are clearly aligned a regresion model would be more suitable than clustering one.

#### 1.2.1 Connections with Supervised Models
The outlier detection problem can be interpreted as a classification problem in which labels are normal or anomaly.

This problem is unsupervised as we don't know the expected result of classification and we should detect it ourselves. The model is usually trained and tested with the same data set as we don't differ on it. This could cause overfitting. Another point of view is that we have the whole data set classified as normal and we filter it to check the training errors which will be the outliers.

### 1.3 The Basic Outlier Detection Models

#### 1.3.1 Feature Selection in Outlier Detection
It is very difficult to perform a feature selection as the problem is unsupervised. Nevertheless a common way of measuring the non-uniformity of a set of unvariate points is the Kurtosis measure.

$z_i = \frac{x_i - \mu}{\sigma}$

$K(z_1 ... z_N) = \frac{\sum_{i=1}^{N}z_i^4}{N}$

This will give a high score if the data is non-uniform. It shows how long is the data spread over the image. This is commonly used to look for outliers in subspaces of lower dimension of the data.

A problem of this measure is that it does not measures well the interaction between atributes when it analyzes the features individually.

#### 1.3.2 Extreme-Value Analysis
This method classifies as outliers points that are too large or too small, in other words you need to detect the tails of the underlying distribution.

This is a very poor idea as the oulier can easily be in the middle values and the normal ones in the tails.

This kind of analysis is normally performed at the end of the algorithm when we have the points already scored.

#### 1.3.3 Probabilistic and Statistical Models
This method tries to learn the parameters of a distribution to try to fit the data so that the ones that don't fit are outliers.

This is a very good way to make a general procedure to find anomalies. The inconvinient of this comes up when we add too many parameters to the models and the data overfits including anomalies as normal points.

#### 1.3.4 Linear Models
These methods model the data along lower-dimensional subspaces with the use of linear correlations. Typically a least-squares fit is used to determine the optimal lower-dimensional hyperplane.

For example in 2-dimensions a linear model of the data points $\{ (x_i,y_i), i\in \{1 , ... , N \}$ in terms of two coefficients a and b may be created as follows:

$y_i = a\cdot x_i + b + \epsilon_i$   $\forall i \in \{1 , ... , N\}$

Where $\epsilon_i$ represents the residual which is the modeling error.

#### 1.3.4.1 Spectral Models
These methods are related to clustering graphs and networks which are referred as spectral models. They are closely related to matrix factorization.

#### 1.3.5 Proximity-Based Models
The idea is to model outliers as points that are isolated from the rest of the data based on similarity or distance functions. They could be classified as clustering, density based and nearest neighbors.

The difference of density based methods and clustering ones is that in clustering we want to segment the data whereas in density based we want to do it with the space.

#### 1.3.6 Information-Theoretic Models
We can also detect outliers as the elements that make the dataset description longer or more difficult, so that if we remove that element the description becomes concise.

#### 1.3.7 High-Dimensional Outlier Detection
The most useful and common thing to do is figure out where those outliers are in the subspaces of the data and then obtain properties on the whole space.

### 1.4 Outlier Ensembles
It is very common in a lot of different areas to combine simple algorithms to obtain a more robust one. This technique in this area is called ensembles.

There are two types of ensembles:
- Sequential ensembles: the algorithms are applied sequentially so that the next algorithm receives the output of the previous. The output of this could be the output of the last simple algorithm or a weighted combination of the results of all the simple algorithms.
- Independent ensembles: different algorithms or different instantiations of the same algorithms are applied to either the whole dataset or portions of it. The results are combined to obtain a more robust one.

#### 1.4.1 Sequential Ensembles
The idea is to apply sequentially a series of algorithms to either the whole dataset or portions of it. We can keep this procedure for a fixed number of iterations or until convergence.
This process could be seen as a refinement of the results data on continuous aproaches.

#### 1.4.2 Independent Ensembles
In this kind of ensembles differents algorithms may be applied on the whole dataset or portions of it but, they do not refine the output of the previous. The output of each of those algorithms are combined to obtain the general output.
This method if commonly used for high dimension problems.

### 1.5 The Basic Data Types for Analysis
#### 1.5.1 Categorical, Text and Mixed Attributes
It is very important to understand which kind of data we have to analyse as the methods we are going to apply are going to be different ones.

#### 1.5.2 When the Data Values have Dependencies
Normally when we have a dataset the data can be partially dependent on other points, for example in time. We can also detect an anomaly by stablishing the relationship with other points in the data. This kind of outliers are referred as contextual outlier or conditional anomalies.
When we find a set of data items that are anomalous as a group we call them a collective anomaly.

#### 1.5.2.1 Times-Series Data and Data Streams
Normally when considering time series the data does not changes significantly in timestamps that are close by or they do it in a smooth way. In this context it is very important the temporal context to identify the anomalies.

If a time series changes smoothly then there are no suspicions that it is an anomaly, in the other hand if the change is produced abruptly then we should be concerned that it could be one.

One of the challenging scenarios with this kind of data is to perform the anomaly detection receiving data in real time.

#### 1.5.2.2 Discrete Sequences
They are like the continuous sequences but they should have a special treatment if they have categorical data.

#### 1.5.2.3 Spatial Data
In this area the continuity and smoothness of the data is very important as well because if we find a peak or discontinuity in the data then it is really easy to figure out the anomalies.

#### 1.5.2.4 Network and Graph Data
