# Notas Outlier Analysis Charu C. Aggarwal 2017

## Cap√≠tulo 1 An Introduction to Oulier Analysis

### 1.1 Introduction
An outlier or anomaly could be defined as an observation that deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanisim.

The outlier detection can be used in: IDS, credit card fraud detection, interesting sensor events, medical diagnosis, law enforcement or earth science.

The normal data points are usually referred as inliers. In some cases the ouliers are not single points but a sequence or group of points that represents by themselves an anomaly.

When detecting anomalies algorithms can give two types of output: outlier scores that gives a score qualifying the outlierness of each point and binary labels indicating whether a point is an outlier or not.

It is important to make the difference between anomaly and outlier. An outlier refers to a point that can be considered as noise or an abnormality and anomaly refers to a special kind of outlier that is of interest of the analyst.

We could fall on the idea of considering the noise as low scored (weak) outliers and anomaly as high scored (strong) ones, but the real barrier between those two is the interest of the analyst.


Unsupervised detection is the one in which we dont have previous examples and normal data to compare with the new ones and supervised detection is the opposite one.

It is clear that the dimensionality, the type of attributes in the data or the ordering are really important when detecting anomalies and should be taken into account.

### 1.2 The Data Model is Everything
The data model chosen to fit our data is crucial as it will give us by methods as k-nearest neighbors or studying the deviation from normal points a score of the outlierness.

It is a really dificult task to choose the model as it is an unsupervised one in which we dont have labeled examples to learn from. Because of this the model is usually chosen by the analyst on what he believes it is the best.

A simple model to qualify a point as an outlier could be the Z-test. This test computes with the obervations $X_1 , ... , X_N$ with mean $\mu$ and standard deviation $\sigma$ the number $Z_i = \frac{|X_i - \mu|}{\sigma}$. An implicit asumption of this is that the model is a Gaussian one. A simple classification could be that if $Z_i \geq 3$ then we classify the point as an anomaly.

If there are not enough data points the mean and deviation cannot be obtained robustly and we should be considering other distribution (Student's one) rather tahn normal one.

The model chosen depends on the data, for example if they are clearly aligned a regresion model would be more suitable than clustering one.

#### 1.2.1 Connections with Supervised Models
The outlier detection problem can be interpreted as a classification problem in which labels are normal or anomaly.

This problem is unsupervised as we don't know the expected result of classification and we should detect it ourselves. The model is usually trained and tested with the same data set as we don't differ on it. This could cause overfitting. Another point of view is that we have the whole data set classified as normal and we filter it to check the training errors which will be the outliers.

### 1.3 The Basic Outlier Detection Models

#### 1.3.1 Feature Selection in Outlier Detection
It is very difficult to perform a feature selection as the problem is unsupervised. Nevertheless a common way of measuring the non-uniformity of a set of unvariate points is the Kurtosis measure.

$z_i = \frac{x_i - \mu}{\sigma}$

$K(z_1 ... z_N) = \frac{\sum_{i=1}^{N}z_i^4}{N}$

This will give a high score if the data is non-uniform. It shows how long is the data spread over the image. This is commonly used to look for outliers in subspaces of lower dimension of the data.

A problem of this measure is that it does not measures well the interaction between atributes when it analyzes the features individually.

#### 1.3.2 Extreme-Value Analysis
This method classifies as outliers points that are too large or too small, in other words you need to detect the tails of the underlying distribution.

This is a very poor idea as the oulier can easily be in the middle values and the normal ones in the tails.

This kind of analysis is normally performed at the end of the algorithm when we have the points already scored.

#### 1.3.3 Probabilistic and Statistical Models
This method tries to learn the parameters of a distribution to try to fit the data so that the ones that don't fit are outliers.

This is a very good way to make a general procedure to find anomalies. The inconvinient of this comes up when we add too many parameters to the models and the data overfits including anomalies as normal points.

#### 1.3.4 Linear Models
These methods model the data along lower-dimensional subspaces with the use of linear correlations. Typically a least-squares fit is used to determine the optimal lower-dimensional hyperplane.

For example in 2-dimensions a linear model of the data points $\{ (x_i,y_i), i\in \{1 , ... , N \}$ in terms of two coefficients a and b may be created as follows:

$y_i = a\cdot x_i + b + \epsilon_i$   $\forall i \in \{1 , ... , N\}$

Where $\epsilon_i$ represents the residual which is the modeling error.

#### 1.3.4.1 Spectral Models
These methods are related to clustering graphs and networks which are referred as spectral models. They are closely related to matrix factorization.

#### 1.3.5 Proximity-Based Models
The idea is to model outliers as points that are isolated from the rest of the data based on similarity or distance functions. They could be classified as clustering, density based and nearest neighbors.

The difference of density based methods and clustering ones is that in clustering we want to segment the data whereas in density based we want to do it with the space.
